"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[9390],{2286(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module4-vla/chapter2-cognitive-planning","title":"Chapter 2: Cognitive Planning with LLMs","description":"Introduction: From Simple Commands to Complex Plans","source":"@site/docs/module4-vla/chapter2-cognitive-planning.mdx","sourceDirName":"module4-vla","slug":"/module4-vla/chapter2-cognitive-planning","permalink":"/physical_ai_and_humanoid_robotics_book/docs/module4-vla/chapter2-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/HuzaifaLk/physical_ai_and_humanoid_robotics_book/tree/main/docs/module4-vla/chapter2-cognitive-planning.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"defaultSidebar","previous":{"title":"Chapter 1: Voice-to-Action","permalink":"/physical_ai_and_humanoid_robotics_book/docs/module4-vla/chapter1-voice-to-action"},"next":{"title":"Chapter 3: Capstone \u2013 The Autonomous Humanoid","permalink":"/physical_ai_and_humanoid_robotics_book/docs/module4-vla/chapter3-capstone-autonomous-humanoid"}}');var o=t(4848),i=t(8453);const r={sidebar_position:2},s="Chapter 2: Cognitive Planning with LLMs",l={},c=[{value:"Introduction: From Simple Commands to Complex Plans",id:"introduction-from-simple-commands-to-complex-plans",level:2},{value:"Natural Language to Task Plans",id:"natural-language-to-task-plans",level:2},{value:"Designing the Prompt",id:"designing-the-prompt",level:3},{value:"Parsing the LLM&#39;s Output",id:"parsing-the-llms-output",level:3},{value:"Sequencing ROS 2 Actions",id:"sequencing-ros-2-actions",level:2},{value:"Error Handling and Replanning",id:"error-handling-and-replanning",level:2},{value:"Summary",id:"summary",level:2}];function h(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-2-cognitive-planning-with-llms",children:"Chapter 2: Cognitive Planning with LLMs"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction-from-simple-commands-to-complex-plans",children:"Introduction: From Simple Commands to Complex Plans"}),"\n",(0,o.jsxs)(n.p,{children:['In the previous chapter, we built a pipeline to translate a simple, direct command like "move forward" into a single robot action. But what if the command is more complex, like "go to the kitchen and get me a soda"? A simple keyword parser can\'t handle this. This type of high-level, multi-step instruction requires a form of "common-sense" reasoning and planning. This is where ',(0,o.jsx)(n.strong,{children:"Large Language Models (LLMs)"})," come in."]}),"\n",(0,o.jsxs)(n.p,{children:['An LLM, such as those from the GPT family, can act as the "cognitive brain" of our robot. By leveraging its vast knowledge, we can use an LLM to take a high-level natural language command and break it down into a structured sequence of executable actions that our robot\'s ROS 2 system can understand. This is the essence of ',(0,o.jsx)(n.strong,{children:"cognitive planning"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"natural-language-to-task-plans",children:"Natural Language to Task Plans"}),"\n",(0,o.jsxs)(n.p,{children:["The core idea is to treat the LLM as a planning service. A ROS 2 node, let's call it the ",(0,o.jsx)(n.code,{children:"CognitivePlannerNode"}),", will be responsible for this."]}),"\n",(0,o.jsx)(n.p,{children:"The workflow looks like this:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["The ",(0,o.jsx)(n.code,{children:"CognitivePlannerNode"})," receives a high-level goal, perhaps from the ",(0,o.jsx)(n.code,{children:"/voice_command_text"})," topic we created in the last chapter."]}),"\n",(0,o.jsxs)(n.li,{children:["It constructs a carefully designed ",(0,o.jsx)(n.strong,{children:"prompt"})," to send to an LLM API (like the OpenAI API)."]}),"\n",(0,o.jsx)(n.li,{children:"The LLM processes the prompt and returns a structured plan."}),"\n",(0,o.jsxs)(n.li,{children:["The ",(0,o.jsx)(n.code,{children:"CognitivePlannerNode"})," parses this plan and begins executing the actions in sequence."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"designing-the-prompt",children:"Designing the Prompt"}),"\n",(0,o.jsxs)(n.p,{children:["The key to getting a useful plan from an LLM is ",(0,o.jsx)(n.strong,{children:"prompt engineering"}),". The prompt must give the LLM enough context to understand the robot's capabilities and the desired output format."]}),"\n",(0,o.jsx)(n.p,{children:"A good prompt for a robotic task planner might include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"A role"}),': "You are a helpful AI assistant for a household robot."']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"The goal"}),': The natural language command from the user (e.g., "go to the kitchen and get me a soda").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"A list of available actions"}),": A description of the ROS 2 actions the robot can perform (e.g., ",(0,o.jsx)(n.code,{children:"navigate_to(waypoint)"}),", ",(0,o.jsx)(n.code,{children:"find_object(object_name)"}),", ",(0,o.jsx)(n.code,{children:"grasp_object(object_id)"}),")."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Output format instructions"}),": A request for the output to be in a machine-readable format, like JSON."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Example Prompt:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:'You are a helpful AI assistant for a household robot. Your task is to take a user\'s command and convert it into a structured plan.\r\n\r\nThe robot has the following available actions:\r\n- navigate_to(waypoint): moves the robot to a named location (e.g., "kitchen", "living_room").\r\n- find_object(object_name): looks for a specific object in the current vicinity.\r\n- grasp_object(object_id): picks up the specified object.\r\n\r\nUser command: "go to the kitchen and get me a soda"\r\n\r\nPlease provide a plan as a JSON array of objects, where each object has an "action" and a "parameter".\n'})}),"\n",(0,o.jsx)(n.h3,{id:"parsing-the-llms-output",children:"Parsing the LLM's Output"}),"\n",(0,o.jsx)(n.p,{children:"Given the prompt above, the LLM might return a JSON response like this:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'[\r\n  {\r\n    "action": "navigate_to",\r\n    "parameter": "kitchen"\r\n  },\r\n  {\r\n    "action": "find_object",\r\n    "parameter": "soda"\r\n  },\r\n  {\r\n    "action": "grasp_object",\r\n    "parameter": "soda_1" \r\n  }\r\n]\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsxs)(n.em,{children:['(Note: The LLM might need to infer an object ID like "soda_1" from the output of the ',(0,o.jsx)(n.code,{children:"find_object"})," action, which adds another layer to the system's complexity.)"]})}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"CognitivePlannerNode"})," would then parse this JSON array. It now has a structured ",(0,o.jsx)(n.strong,{children:"task plan"})," that it can execute step-by-step."]}),"\n",(0,o.jsx)(n.h2,{id:"sequencing-ros-2-actions",children:"Sequencing ROS 2 Actions"}),"\n",(0,o.jsx)(n.p,{children:"Once the plan is parsed, the planner node needs to execute it. Since each step might be a long-running ROS 2 action, the planner must call each action, wait for it to complete successfully, and then move on to the next one."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Conceptual Python Node Snippet:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# (Conceptual code, not a complete running example)\r\nclass CognitivePlannerNode(Node):\r\n    def __init__(self):\r\n        super().__init__('cognitive_planner_node')\r\n        # ... subscription to command topic ...\r\n        # ... action clients for navigate_to, find_object, grasp_object ...\r\n\r\n    def command_callback(self, msg):\r\n        command_text = msg.data\r\n        prompt = self.create_llm_prompt(command_text)\r\n        plan_json = self.call_llm_api(prompt)\r\n        task_plan = self.parse_plan(plan_json)\r\n        self.execute_plan(task_plan)\r\n\r\n    def execute_plan(self, plan):\r\n        for task in plan:\r\n            self.get_logger().info(f\"Executing task: {task['action']} with param: {task['parameter']}\")\r\n            \r\n            # This is a simplified execution loop\r\n            if task['action'] == 'navigate_to':\r\n                # Call the navigate_to action and wait for completion\r\n                future = self.nav_action_client.send_goal_async(...)\r\n                # rclpy.spin_until_future_complete(...)\r\n                result = future.result()\r\n                if not result.success:\r\n                    self.get_logger().error('Navigation failed!')\r\n                    self.handle_error(plan, current_task=task)\r\n                    return # Stop the plan\r\n            \r\n            # ... elif for other actions ...\r\n        \r\n        self.get_logger().info('Plan executed successfully!')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"error-handling-and-replanning",children:"Error Handling and Replanning"}),"\n",(0,o.jsxs)(n.p,{children:["What happens if a step in the plan fails? For example, the ",(0,o.jsx)(n.code,{children:"navigate_to"})," action fails because the path is blocked, or the ",(0,o.jsx)(n.code,{children:"find_object"})," action fails because there is no soda in the kitchen."]}),"\n",(0,o.jsxs)(n.p,{children:["A robust cognitive planner must include ",(0,o.jsx)(n.strong,{children:"error handling"}),". When a step fails, the planner can:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Stop"}),": Abort the plan and report the failure to the user."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Retry"}),": Attempt the failed action again a few times."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Replan"}),": This is the most intelligent option. The planner can call the LLM again with a new prompt that includes the context of the failure."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Example Replanning Prompt:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:'... (same initial context) ...\r\n\r\nUser command: "go to the kitchen and get me a soda"\r\n\r\nThe robot attempted the following plan but failed:\r\n1. navigate_to("kitchen") -> SUCCESS\r\n2. find_object("soda") -> FAIL: Object "soda" not found.\r\n\r\nPlease provide a new plan to achieve the original goal. You can use the \'say(text)\' action to communicate with the user.\n'})}),"\n",(0,o.jsx)(n.p,{children:"The LLM might then generate a new plan:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'[\r\n  {\r\n    "action": "say",\r\n    "parameter": "I could not find a soda in the kitchen. Is there somewhere else I should look?"\r\n  }\r\n]\n'})}),"\n",(0,o.jsx)(n.p,{children:"This ability to reason about failure and generate alternative plans is what makes LLM-based planners so powerful and flexible."}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This chapter explored how to use Large Language Models (LLMs) to add a layer of cognitive planning to our robot. We learned how to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Use ",(0,o.jsx)(n.strong,{children:"prompt engineering"})," to make an LLM act as a task planner."]}),"\n",(0,o.jsxs)(n.li,{children:["Convert a high-level natural language command into a structured, machine-readable ",(0,o.jsx)(n.strong,{children:"task plan"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:["Write a ROS 2 node that can ",(0,o.jsx)(n.strong,{children:"sequence"})," the execution of the plan's actions."]}),"\n",(0,o.jsxs)(n.li,{children:["Consider strategies for ",(0,o.jsx)(n.strong,{children:"error handling and replanning"})," when things go wrong."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"In the final capstone chapter, we will see how this cognitive planning system integrates with the perception and navigation stacks from the previous modules to create a truly autonomous humanoid robot."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>s});var a=t(6540);const o={},i=a.createContext(o);function r(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);